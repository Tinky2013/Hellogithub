---
layout:     post                    # 使用的布局（不需要改）
title:      为什么正则项能够减轻过拟合               # 标题 
subtitle:    #副标题
date:       2019-11-20             # 时间
author:     WZY                      # 作者
header-img: img/post-bg-universe.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
mathjax: true
tags:                               #标签
    - Machine Learning
--- 

# 为什么正则项能够减轻过拟合

在机器学习中常常说加入正则化项就能缓解过拟合现象，但是为什么会缓解呢，其中的原理还是值得深入思考一下的。

## 1 从最小二乘估计和岭估计出发理解正则化

### 1.1 概述

对于一般的线性模型 $$y=X\beta$$ 我们可以对于模型的参数进行估计，我们定义：

* 最小二乘估计:：

$$\hat{\beta}_{OLS}=arg\ min_\beta ||y-X\beta||^2$$

* 岭估计：

$$\hat{\beta}_{Ridge}=arg\ min_\beta ||y-X\beta||^2+\lambda||\beta||^2$$

可以看到，岭估计带有$\lambda$正则化参数，可以求出：

$$\hat{\beta}_{OLS}=(X'X)^{-1}X'y$$

$$\hat{\beta}_{Ridge}=(X'X+\lambda I)^{-1}X'y$$

而以下这条等式是可以在数学上给予证明的（注意这里可以取得严格的小于号）

$$||\hat{\beta}_{Ridge}||<||\hat{\beta}_{OLS}||$$

加入正则化的作用其实是让参数向量的长度变短（在英文上用**Shrinkage**这个词语似乎更为准确），于是$\hat{\beta}$中的某些分量在平均意义上会比加入正则化项之前更小（某些正则项的情况下甚至会被压缩到零）。

在回归分析中，类似的有岭估计存在性定理：存在$\lambda$使得

$$MSE(\hat{\beta}_{Ridge}^\lambda)< MSE(\hat{\beta}_{OLS})$$

也就是说，存在特定的岭参数，使得岭估计在均方意义下由于最小二乘估计。

### 1.2 shrinkage的意义

在回归分析中，当自变量有较强的负相关性的时候，最小二乘估计的结果往往会很差。这个时候岭估计通过牺牲无偏性达到更小均方误差的效果。可以看到，一方面这种方法可以消除噪声，达到更小的泛化误差；另一方面对于多重共线性的情况，可以消除关联的特征去提高模型的稳定性，并且能够避免多重解的出现。

### 1.3 L1正则化和L2正则化

从式子上看L1和L2正则化的区别很显然，前者使用L1范数：

$$\sum_{i=1}^n |w_i|$$

而后者使用L2范数：

$$\sum_{i=1}^n ||w_i||^2$$

解释L1正则化和L2正则化区别的时候有一张非常经典的图，这张图对应的是模型只有$w_1$和$w_2$参数的时候的情形（更多参数则在更高维的空间中表达）。

![](https://github.com/Tinky2013/Machine-Learning-Lab/raw/master/img/L1L2.jpg)

L2正则化（又称为weight decay），可以使得参数非常趋向于零但是不等于零，而L1正则化则会使得某些参数变为零，即趋向于产生少量的特征。

从上图我们也可以直观看出，对于L2正则化，设定的模型特征使得和红色圆的切点不会落在坐标轴上，因此对应的权重$w_i$也不会变为零，而L1正则化中切点可以是方形的角点（二维情形下），这个时候便会有权重参数被压缩到零。如图中的情形，$w_2$就被压缩为零。

## 2 正则反映了什么

通常我们说正则项是用来惩罚模型使得模型的复杂度不要太高，但是从方程上来看，正则项实际上是给方程加上了一种**约束**，这种约束一般是某种范数最小，通过施加约束，能够使得解空间大大缩小。

另外正则化也是一种**平衡**的策略，在训练的时候我们既希望较好地拟合训练数据，又希望模型参数不要太大，因此正则化参数的大小实际上也反应了这种平衡的决策。

## 3 从贝叶斯的角度看正则化

正则项也可以通过贝叶斯的角度来解释。其中L1正则对应的便是参数$w$的Laplace先验，L2正则对应的是Gaussian先验，而我们在线性模型中的普通线性回归就可以看做是无信息先验（或者说Uniform先验）。

比如L2的情况下，损失函数可以写为：

$$E(w)=\frac{\beta}{2}\sum_{n=1}^N(y(x_n,w)-t_n)^2+\frac{\alpha}{2}w^Tw$$

其中我们加的先验分布以及通过先验推导出的后验为：

$$p(w|\alpha)=N(w|0,\alpha^{-1}I)=(\frac{\alpha}{2\pi})^{(M+1)/2}exp(-\frac{\alpha}{2}w^Tw)$$

$$p(w|x,t,\alpha,\beta)\propto p(t|x,w,\beta)p(w|\alpha)$$

我们给模型的参数添加量一个协方差为$\alpha$的零均值高斯先验，$\alpha$越小，那么参数的协方差就越大，从而弱化对于参数的约束。

