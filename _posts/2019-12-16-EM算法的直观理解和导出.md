---
layout:     post                    # 使用的布局（不需要改）
title:      EM算法的直观理解和导出               # 标题 
subtitle:    #副标题
date:       2019-12-16             # 时间
author:     WZY                      # 作者
header-img: img/post-bg-universe.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
mathjax: true
tags:                               #标签
    - Machine Learning
--- 

# EM算法的直观理解和导出

## 1 算法的引出

在样本服从某个分布的假设下，我们可以利用极大似然的方法进行估计，然而，如果样本是来源于多个不同分布的，即不知道每个样本是从哪个分布中抽取的，这个时候，对于每一个样本，需要估计的内容便包括其来自的分布以及该分布的参数。

这个时候，简单的极大似然估计不再适用：只有当我们知道了样本中哪些个体是属于同一个分布的时候，我们才能够对这个分布的参数作出比较可靠的参数估计，如利用极大似然法；但当来自不同分布的样本混在一起，不知道每个个体属于哪个分布的时候，便无法估计分布的参数。

另一方面，只有对于每一个分布的参数都做出了比较合理的估计，才能较为准确推断出每个样本是属于哪个分布的，此时参数估计所面临的便是一个相互依赖的循环问题，而EM算法便能够解决这样的问题，其基本思路便是，先任意初始化一个参数的值，再根据分布估计的变化调整参数下一步的值，如此迭代下去，最终就会收敛到一个解。这个解便可以作为待估参数的估计。

## 2 一个例子
### 2.1 硬币类型已知
理解极大似然估计的一个经典的例子便是双硬币问题。假设有两枚硬币，第一枚硬币为不均匀硬币，其正面朝上概率为$P_{real1}=0.4$，第二枚硬币为均匀硬币，其正面朝上概率为$P_{real2}=0.5$。

![](https://github.com/Tinky2013/Machine-Learning-Lab/raw/master/img/7684%20EM/1.jpg)

假设我们不知道每一枚硬币正面向上的概率，需要根据实验结果对齐进行估计。现在任意选择硬币，总共抛掷五次，记录每次抛掷的硬币种类和结果，如下表：


|  coin     |      result       |        sum      |
| ---|---|---|
|    使用第1枚硬币 |    正面，正面，反面，正面，反面    |      3正2反 |
|    使用第2枚硬币 |    反面，反面，正面，正面，反面     |      2正反 |
|   使用第1枚硬币 |    正面，反面，反面，反面，反面     |     1正4反 |
|  使用第2枚硬币 |    正面，反面，反面，正面，正面    |      3正2反 | 
|  使用第1枚硬币 |    反面，正面，正面，反面，反面    |      2正3反  |


根据该表格，我们便能利用极大似然法推断出硬币的参数$\theta$，在这里即为正面向上的概率$P_{real1}$和$P_{real2}$。

$$ p_1=\frac{3+1+2}{15}=0.4$$

$$  p_2=\frac{2+3}{10}=0.5$$

### 2.2 硬币类型未知
当不存在“隐变量”的时候，我们能通过极大似然估计得到参数较为准确的估计值。
然而，当我们不知道每一次用的是哪一种硬币的时候，这种极大似然估计便失去了作用，需要用另外一种方法来解决。此时每次抛掷的硬币的种类便成为了参数估计中的“隐变量”。

|coin     |       result       |        sum      |
|---|---|---|
|Type Unknown |    正面，正面，反面，正面，反面    |     3正2反     |
|Type Unknown |    反面，反面，正面，正面，反面     |      2正3反   |
| Type Unknown |    正面，反面，反面，反面，反面     |      1正4反  |
|Type Unknown |    正面，反面，反面，正面，正面    |      3正2反    |
|Type Unknown |    反面，正面，正面，反面，反面    |      2正3反    |

在这种情况中，问题的隐变量可以记作$\textbf{z}=(z_1,z_2,z_3,z_4,z_5)$，其中$z_i$表示每次选用的是哪一枚硬币。

显然，假若知道了每次抛掷的硬币种类，便可以用极大似然法估计硬币的参数，另一方面，假若知道了硬币的参数，也能够推断出每次最有可能抛掷硬币的种类。

要求解此问题，首先需要初始化参数，比如这里利用$\mathbf{\theta^0}=(p_1^0,p_2^0)=(0.7,0.3)$，通过计算期望选择每一轮最有可能使用的硬币。比如对于第一轮而言，计算公式如下：

$$p_1=0.2^3×(1-0.2)^2=0.00512$$

$$p_2=0.7^3×(1-0.7)^2=0.03087$$

$$coin1=\frac{0.00512}{0.00512+0.03087}=0.14$$

$$ coin2=1-0.14=0.86$$

对于所有抛掷的结果通过以上计算统计于表格中：

| round    |  sum |  if coin 1    |    if coin 2   | expect coin 1| expect coin 2|
|---|---|---|---|---|---|
|    1 |    3+,2-    | 0.00512 | 0.03087 | 0.14|0.86|
|    2 |    2+,3-    | 0.02048 | 0.01323 | 0.61|0.39|
|    3 |    1+,4-    | 0.08192 | 0.00567 | 0.94|0.06|
|    4 |    3+,2-    | 0.00512 | 0.03087 | 0.14|0.86|
|    5 |    2+,3-    | 0.02048 | 0.01323 | 0.61|0.39|

根据所有实验的统计结果，可以估计出硬币相应的参数，以第一枚硬币为例，其正面向上的概率可以如此算出：


$$p_1^+=0.14*3+0.61*2+0.94*1+0.14*3+0.61*2=4.22$$

$$p_1^-=0.86*2+0.39*3+0.06*4+0.86*2+0.39*3=7.98$$

$$p_1^{(1)}=\frac{p_1^+}{p_1^++p_1^-}=0.35$$

可以看出，迭代一次之后$p_1$的估计值$p_1^{(1)}$会比初始值$p_1^{(0)}$更加接近于真实值(0.4)，这样的迭代有两个重要的特点：

第一，每次迭代的参数值将更加接近于真实值；第二，迭代的参数值不一定收敛至真实值。这两个特性将在后文给予证明。

## 3 EM算法的导出
### 3.1 符号和定义
上面对于硬币参数进行估计的思路正是EM算法的雏形，其中，每轮抛掷的正面和反面的总数是我们可以观测到的，可以称之为“观测变量”或“显变量”；而问题中我们无法观测到的，就是每次抛掷所使用的硬币种类，这里称之为“隐变量”。而我们需要估计的参数，即我们最终所希望得到的，是每一枚硬币正面朝上的概率，即“参数集”。

由此可以引出求解此问题的EM算法，具体定义如下：

* $\textbf{Z}=(z_1,z_2,...,z_m)$：隐变量集

* $\textbf{X}=(x_1,x_2,...,x_n)$：显变量集或观测变量集

* $\mathbf{\Theta}=(\theta_1,\theta_2,...,\theta_k)$：参数集

### 3.2 算法求解

为了求解参数估计问题，首先极大化似然函数：

$$L(\Theta |X,Z)=logP(X,Z|\Theta)$$

由于出现了隐变量$\textbf{Z}$，需要现对于隐变量求期望（这里假定是离散型随机变量）：

$$logP(X|\Theta)=log\sum_{Z}P(X,Z|\Theta)=log(\sum_Z P(X|Z,\Theta)P(Z|\Theta))$$

接下来在通过对于对数似然函数求偏导的时候会遇到困难，由于在对数中出现了求和符号（连续型随机变量将会出现积分符号），最大化似然函数的计算会相当复杂。

而EM算法利用参数向量初始化以及不断迭代的方式，能够使得这个问题得以解决。

EM算法的核心步骤有如下两步：


* $\textbf{E step}$：如果参数向量$\mathbf{\Theta}$已知，可以通过已有数据推断出最优的隐变量$\textbf{Z}$的取值。

$$Q(\Theta|\Theta^i)=E_{Z|X,\Theta^i}L(\Theta|X,Z)$$

* $\textbf{M step}$: 如果隐变量$\textbf{Z}$的取值已知，则可以对于参数向量$\textbf{\Theta}$进行极大似然估计。 


$$ \Theta^{i+1}={arg\ max}_\Theta Q(\Theta|\Theta^i)$$

EM算法本质上就是以上两个步骤的不断迭代，直至收敛。回归上午的双硬币问题，可以看出，将初始化即每次迭代出的参数解$(p_1^{(i)},p_2^{(i)})$带入求出每种硬币的期望，对应的正好是这里的E步；通过求出的硬币的最可能组合$\textbf{Z}=(z_1,z_2,...)$进行极大似然估计求出新一轮的参数向量，对应的正好是这里的M步。

![](https://github.com/Tinky2013/Machine-Learning-Lab/raw/master/img/7684%20EM/3.jpg)

### 3.3 EM算法的本质——局部下界逼近

EM算法通过迭代的方式逐步近似最大化似然函数，假设第i次迭代之后的$\theta$值为$\theta^{(i)}$，那么希望估计新的值$\theta$使得$L(\theta)$增加。因此，考虑两者的差值：
$L(\Theta)-L(\Theta^i)$，由下面的Jensen不等式：


$$log\sum_j\mu_j y_j \geq \sum_j\mu_j log y_j$$

$$\mu_j\geq 0,\sum_j\mu_j=1$$

求得此差值的下界：


$$\begin{aligned}    L(\Theta)-L(\Theta^i) &= log(\sum_Z P(X|Z,\Theta)P(Z|\Theta))-logP(X|\Theta^i)\\
      &= log(\sum_Z P(Z|X,\Theta^i)\frac{P(X|Z,\Theta)P(Z|\Theta)}{P(Z|X,\Theta^i)})-logP(X|\Theta^i)\\
      &\geq \sum_Z P(Z|X,\Theta^i)log(\frac{P(X|Z,\Theta)P(Z|\Theta)}{P(Z|X,\Theta^i)})-logP(X|\Theta^i)\\
      &=\sum_Z P(Z|X,\Theta^i)log(\frac{P(X|Z,\Theta)P(Z|\Theta)}{P(Z|X,\Theta^i)P(X|\Theta^i)})
      \end{aligned}$$

  这里，令：

$$B(\Theta,\Theta^i)=L(\Theta^i)+\sum_Z P(Z|X,\Theta^i)log(\frac{P(X|Z,\Theta)P(Z|\Theta)}{P(Z|X,\Theta^i)P(X|\Theta^i)})$$

  显然能够得到：

$$L(\Theta)\geq B(\Theta,\Theta^i)$$

  这说明函数$B(\theta,\theta^i)$是$L(\theta)$的一个下界，而又有：

$$L(\Theta^i)=B(\Theta^i,\Theta^i)$$

  这说明能够使得$B(\Theta^i,\Theta^i)$增大的$\theta$也能够使得$L(\theta)$增大。因此，在下一轮迭代中，选择$\theta^{i+1}$使得B函数达到最大，即：

$$\Theta^{i+1}=arg\ max_\Theta B(\Theta,\Theta^i)$$

  于是可以写出EM算法的一次迭代形式，上式可以化为：

$$ \begin{aligned}
      \Theta^{i+1}&=arg\ max_\Theta B(\Theta,\Theta^i)\\
      &=arg\ max_\Theta(L(\Theta^i)+\sum_Z P(Z|X,\Theta^i)log(\frac{P(X|Z,\Theta)P(Z|\Theta)}{P(Z|X,\Theta^i)P(X|\Theta^i)}))\\
      &=arg\ max_\Theta(\sum_Z P(Z|X,\Theta^i)log(P(X|Z,\Theta)P(Z|\Theta)))\\
      &=arg\ max_\Theta(\sum_Z P(Z|X,\Theta^i)logP(X,Z|\Theta))\\
      &=arg\ max_\Theta Q(\Theta,\Theta^i)
    \end{aligned}$$

因此，EM算法可以理解为通过不断求解下界的逼近对数似然函数极大值的方法，如下图也可以对于EM算法进行直观的解读，在点$\theta=\theta^i$处函数$L(\theta)$和$B(\theta,\theta^i)$相等。

而EM算法需要找到下一个参数估计$\theta^{i+1}$使得B函数最大化(也等价于Q函数最大化)，同时在$\theta^{i+1}$处重新计算Q函数的值，进行下一次迭代，在这个过程中对数似然函数不断增大，能够保证每次迭代必定更为接近参数的真实值。
另外从图中也可以直观看出，EM算法不能够保证找到全局最优值，这依赖于初始值的选取。

![](https://github.com/Tinky2013/Machine-Learning-Lab/raw/master/img/7684%20EM/2.jpg)

## 4 EM算法与其他方法的联系
在机器学习中，EM算法不是一个具体的模型，而是一种较为一般的方法，常用于统计推断中隐变量的估计。然而，EM算法还和众多的模型或方法有着密切的联系，许多问题的求解都需要涉及到EM。

### 4.1 用EM求解GMM
GMM，即高斯混合模型(Gaussian mixture model)在生活中非常常见，对于来自不同高斯分布的样本，如果希望通过样本的观测值来估计这些高斯分布的参数，那么需要求解的就是GMM问题。

如图，我们所抽取的样本有可能是来源于多个高斯分布，然而由于样本是混在一起的，无法通过直接的极大似然估计去求得每个高斯分布的具体参数。

![](https://github.com/Tinky2013/Machine-Learning-Lab/raw/master/img/7684%20EM/4.jpg)

一般来说，高斯混合模型的定义如下：

$$P(y|\Theta)=\sum_{k=1}^K\alpha_k\phi(y|\theta_k)$$

其中$\alpha_k$是系数，有$\alpha_k\geq 0$,$\sum_{k=1}^K \alpha_k=1$；$\phi(y\|\theta_k)$是第k个高斯分布的密度函数，即：

$$ \phi(y|\theta_k)=\frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{(y-\mu_k)^2}{2\sigma_k^2})$$

因此，在高斯混合分布中使用EM模型时，需要估计的参数为$\Theta=(\alpha_1,...\alpha_k;\theta_1,...\theta_k)$，其中$\theta_k=(\mu_k,\sigma_k)$。而隐变量就是每个观测值是来源于哪个分模型的。

GMM在实际中的应用非常广泛，在数据的聚类上，很多情况下，使用单个高斯分布并不能很好地拟合实际的情况，事实上，有时候反而会适得其反，造成对于总体参数估计的偏差。如下图二维的情况，数据实际上来源于两个二元高斯分布，这个时候使用单个高斯分布去进行数据拟合往往得不到好的效果，而使用混合高斯便能较为清晰地分析出实际分布的情况。

![](https://github.com/Tinky2013/Machine-Learning-Lab/raw/master/img/7684%20EM/5.jpg)![](https://github.com/Tinky2013/Machine-Learning-Lab/raw/master/img/7684%20EM/6.jpg)

### 4.2 EM和k-means的联系
k-means，即k均值聚类，是一种常见的聚类算法。这种聚类算法也是通过迭代求解的，开始时，k-means随机选取K个对象作为初始的聚类中心，然后计算每个样本点与各子聚类中心之间的距离，从而把每个样本点分配给距离它最近的聚类中心。

每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算。此过程将会不断迭代进行直到满足终止条件。

k-means求解的优化问题可以写成如下的形式：

$$min.\ J=\sum_{n=1}^N\sum_{k=1}^K r_{nk}||\textbf{x}_n-\mathbf{\mu}_k||^2$$

$$r_{nk}=\begin{cases}1& k=arg\ min_j||\textbf{x}_n-\mathbf{\mu}_j||^2\\
  0& \text{otherwise.}\end{cases}$$

将其转化为求解二次规划问题：

$$2\sum_{n=1}^N r_{nk}(\mathbf{x}_n-\mathbf{\mu}_k)=0$$

由此可以得到初始化的聚类中心：

$$\mathbf{\mu}_j=\frac{\sum_n r_{nk}\mathbf{x}_n}{\sum_n r_{nk}}$$

k-means的求解需要用到EM算法，如下图，初始化k-means需要先随机确定几个聚类中心(如图a)，这便是EM算法中参数初始化的部分。一旦将所有类别的中心$\mu_i$确定，即可根据$\mu_i$进行参数估计(如图b)，根据已知的聚类中心推断隐变量，即每个数据点属于的类别。

接着，在知道类别之后，又可以通过已经聚好的类，对于$\mu_i$进行极大似然估计(如图c)，从而确定新的聚类中心。在k-means求解中，图(b)即对应E步，(c)对应M步。通过两个步骤反复迭代，最终能够求解k-means问题。

![](https://github.com/Tinky2013/Machine-Learning-Lab/raw/master/img/7684%20EM/7.jpg)
