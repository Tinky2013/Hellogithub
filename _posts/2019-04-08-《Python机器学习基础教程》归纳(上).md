---
layout:     post                    # 使用的布局（不需要改）
title:      ML with Python 1               # 标题 
subtitle:   《Python机器学习基础教程》归纳（上） #副标题
date:       2019-04-08             # 时间
author:     WZY                      # 作者
header-img: img/page.png    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Machine Learning
---
此系列文章提炼《Python机器学习基础教程》最核心要点
***

## 第一章 引言
### 一、熟悉任务
1、确定回答的问题
2、表示成机器学习的问题
3、收集的数据是否足够表示这类问题
4、提取了哪些特征，能否实现正确预测
5、如何衡量成功
6、解决方案与研究或商业产品有哪些是互相影响的
### 二、必要的工具
### 三、基本操作
**1、观察数据**
* 1）训练测试集划分
```python
X_train,X_test,y_train,y_test=train_test_split(data['data'],data['target'])
#其中data为导入数据文件的名称，[]中填写表格列名称
```
* 2）可视化观察
```python
iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)
grr=scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o',hist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)
#利用DataFrame创建散点图矩阵
```

**2、构建模型（k-近邻为例）**
* 1）调用模型
```python
from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier(n_neighbors=1)
#scikit-learn所有机器学习模型都是在各自的类中实现的，这些类被称为Estimator类，k临近算法是在neighbors模块的KNeighborsClassifier类中实现的，因此，我们需要这样两行代码
```
* 2）模型拟合
```python
knn.fit(X_train,y_train)
#需要调用knn对象的fit方法
```

**3、作出预测**
```python
prediction = knn.predict(X_new)#X_new是新数据（nparray）
print("Prediction: {}".format(prediction))
```
**4、评估模型**
* 1）方法一：预测再对比
	 ```python
	y_pred = knn.predict(X_test)
	print("Test set score: {:.2f}".format(np.mean(y_pred == y_test)))
	```
* 2）方法二：用score方法
	```python
	print("Testscore:{:.2f}",format(knn.score(X_test,y_test)))
	```


## 第二章 监督学习
### 一、概述
### 二、监督学习算法
**1、k近邻**
* 1）k近邻分类
	* A.基本操作
		```python
		from sklearn.neighbors import KNeighborsClassifier
		clf = KNeighborsClassifier(n_neighbors=3)
		clf.fit(X_train, y_train)
		```
	* B.模型复杂度与泛化能力关系
		* 使用更少邻居对应更高的模型复杂度
		* 代码绘制精度与n_neighbors关系图
			* 利用循环探索1-10
			* 每一步记录训练集与泛化精度（两个accuracy初始化为[]）
				```python
				training_accuracy.append(clf.score(X_train, y_train))
				test_accuracy.append(clf.score(X_test, y_test))
				```
			* 利用plt.plot可视化
* 2）k近邻回归
	* A.基本操作
		```python
		from sklearn.neighbors import KNeighborsRegressor
		reg = KNeighborsRegressor(n_neighbors=3)
		reg.fit(X_train, y_train)
		```
	* B.分析
	单一邻居训练集中每个点对预测结果都有显著影响（图像经过所有预测点）
	邻居越多边界越平滑（但训练得分下降）
* 3）优缺点与参数
	* 参数
	邻居个数：一般3-5较好
	距离度量：默认欧氏距离
	* 优点：容易理解，不需要过多调参
	* 缺点：特征多或样本数很大效果不好，稀疏数据集效果不好，预测慢

**2、线性模型**
* 1）用于回归的线性模型
	* A.概述
	预测结果：单一特征-直线；二维特征-平面；多维特征-超平面
	* B.线性回归（普通最小二乘法）
		* a.基本操作
			```python
			from sklearn.linear_model import LinearRegression
			lr = LinearRegression().fit(X_train, y_train)
			```
		* b.分析
		优缺点：没有参数，但无法控制模型的复杂度
		过拟合风险小，高维数据集强大
	* C.岭回归
		* a.基本操作
			```python
			from sklearn.linear_model import Ridge
			ridge = Ridge().fit(X_train, y_train)
			```
		* b.参数
			* 限制系数α
			数值越大对系数w限制越大（使其趋于零）
			增大α会降低训练集性能但可能提高泛化性能
		* c.分析
		岭回归是L1正则化的（训练分数整体低于线性回归）
		少量数据集分数高，数据多时岭回归和线性回归可能有同样的性能（正则化变得不那么重要）
	* D.lasso
		* a.基本操作
			```python
			from sklearn.linear_model import Lasso
			lasso = Lasso().fit(X_train, y_train)
			```
		* b.参数
			* 限制系数α
			数值减小可以拟合更复杂的模型
			太小会消除正则化的效果
		* c.分析
		lasso是L2正则化的
		更容易解释（只选择了一部分输入特征，α较大时大部分系数都是零）
		sklearn中ElasticNet结合lasso和Ridge的惩罚项
* 2）用于分类的线性模型
	* A.概述
	根据函数值正负分类（分类的线性模型决策边界是输入的线性函数）
	学习线性模型算法的区别
	系数和截距的特定组合对训练数据拟合度好坏的度量方法（损失函数的选择）
	是否使用正则化，以及使用哪种正则化方法
	* B.Logisitic回归\线性支持向量机（线性SVM）
		* a.基本操作
			```python
			from sklearn.linear_model import LogisticRegression
			clf = LogisticRegression().fit(X, y)
			from sklearn.svm import LinearSVC
			clf = LinearSVC().fit(X, y)
			```
		* b.参数
			* 权衡参数C
			C的值越大，对应正则化越弱（拟合度提高）
			较小的C值适应大多数数据点，较大的C值强调每个数据点都正确分类
* 3）用于多分类的线性模型
	* A.概述
	对每个类别都学习一个二分类模型
	测试点上运行所有二分类器预测，对应类别分数最高胜出

**3、朴素贝叶斯分类器**
* 1）特点
训练速度快，但泛化性能比线性模型差
通过单独查看每个特征来学习参数
* 2）分类
	* A.GaussianNB
	可应用于任意连续数据，主要用于高维数据
	* B.BernoulliNB
	假定输入数据为二分类数据（适于文本分类）
	参数α：控制复杂度（越大平滑化越强，模型复杂度越低）
	广泛用于稀疏计数数据
	* C.MultinomialNB
	假定输入数据为计数数据（适于文本分类）
	参数α：同理
	广泛用于稀疏计数数据
	
**4、决策树与决策树集成**
* 1）决策树概述
	* A.构造树原理：学习if-else问题（测试）
	* B.控制决策树复杂度（防止过拟合策略）
	预剪枝：及早停止树的生长（限制条件：最大深度、叶节点最大数、一个节点数据最小数目[防止继续划分]）
	后剪枝：构造完后删除或折叠信息量很少的节点
* 2）分类
	* A.决策树（分类）
		* a.具体操作
			```python
			from sklearn.tree import DecisionTreeClassifier
			tree = DecisionTreeClassifier(max_depth=4,random_state=0)
			tree.fit(X_train, y_train)
			```
		* b.参数
			* max_depth：限制树的深度（只可连续问的问题数）
			* max_leaf_nodes
			* min_samples_leaf
		* c.分析
			* 查看特征重要性（每个特征对树决策的重要性）
				```python
				print("Feature importances:\n{}".format(tree.feature_importances_))
				```
			* 特征重要性可视化（核心代码）
				```python
				n_features = cancer.data.shape[1]
				plt.barh(range(n_features), model.feature_importances_, align='center')
				plt.yticks(np.arange(n_features), cancer.feature_names)
				```
			* 优缺点
			模型容易可视化理解
			算法完全不受数据缩放影响（预处理时无需归一化或标准化）
	* B.决策树（回归）
		* a.具体操作
		```python
		from sklearn.tree import DecisionTreeRegressor
		tree = DecisionTreeRegressor().fit(X_train, y_train)
		```
		* b.特点
		不能外推，不能再训练数据范围之外预测
		其余与分类同理
	* C.随机森林
		* a.具体操作
			```python
			from sklearn.ensemble import RandomForestClassifier
			forest = RandomForestClassifier(n_estimators=5, random_state=2)
			forest.fit(X_train, y_train)
			```
		* b.参数
			* n_jobs：使用内核数量
			* random_state：需要结果可重现时需固定此参数
			* n_estimators：在时间/内存允许的情况下尽量多
			* max_features：决定每棵树的随机性大小，较小的值可以降低过拟合（一般来说分类设为sqrt(n_feature)，回归设为n_feature）
			* max_depth、max_leaf_nodes等
		* c.分析
		最广泛方法（一般无需反复调参，也无需对数据进行缩放）
		高维稀疏数据表现不好（如文本）
	* D.梯度提升回归树（梯度提升机）
		* a.具体操作
			```python
			from sklearn.ensemble import GradientBoostingClassifier
			gbrt = GradientBoostingClassifier(random_state=0)
			gbrt.fit(X_train, y_train)
			```
		* b.参数
			* 含有随机森林所有参数（但此处过大的n_estimators可能导致过拟合，梯度提升的max_depth一般不超过5）
			* learning_rate：学习率，纠正前一棵树错误的强度（降低学习率可能可以提高泛化性能）
		* c.分析
		采用连续的方式构造树，默认强预剪枝而非随机化
		梯度提升思想：合并许多简单的模型（弱学习器：深度较小的树）
		对参数设置更为敏感（但正确的参数会使得精度更高）

**5、核支持向量机**
* 1）基本操作
	```python
	from sklearn.svm import LinearSVC
	linear_svm = LinearSVC().fit(X, y)
	```
* 2）核技巧
	* 核技巧：添加特征在更高维空间中学习分类器，而不需计算非常大的数据表示
	* 原理：直接计算扩展特征表示中数据点之间的距离（内积）
	* 分类
	多项式核：一定阶数内计算原始特征所有可能的多项式
	高斯核：无限阶可能的多项式（阶数越高重要性越小）
	* 支持向量
	定义：位于类别之间边界上的点（对于定义决策边界很重要）
	分类决策：基于与支持向量的距离以及训练过程中学到的支持向量的重要性
* 3）参数
	* gamma：高斯核宽度（点与点“靠近”是多大距离），值越大模型越复杂，决策边界更加关注单个点
	* C：正则化参数（限制每个点的重要性），也是越大越复杂，两个参数一般同时调节
* 4）SVM特点
	* 对参数设定和数据缩放非常敏感
		```python
		min_on_training=X_train.min(axis=0)
		range_on_training=(X_train-min_on_training).max(axis=0)
		X_train_scaled=(X_train-min_on_training)/range_on_training
		```
	* 数据量十万以上时运行时空需求大

**6、神经网络（深度学习）**
* 1）基本操作（多层感知机 MLP）
	```python
	from sklearn.neural_network import MLPClassifier
	mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)
	```
* 2）参数
	* hidden_layer_sizes
	层数和单元，如[10,10]表示两层每层十个单元
	单一隐层时决策边界由隐单元个数的直线段组成
	更加平滑的边界：更多隐层和隐单元
	* activation：非线性函数（relu默认，tanh更平滑）
	* alpha：L2惩罚参数，默认值很小（弱正则化）
	* solver：如何学习模型，默认'adam'（lbfgs：鲁棒性好，但大数据集时间长；sgd：更专业，包含其他参数）
* 3）特点
需要长时间训练和预处理数据
特征具有相似含义的数据集上效果最好（特征种类很不同基于树的模型更好）
### 三、不确定估计
**1、决策函数**
* 1）使用方法
	```python
	model.decision_function(X_test)
	#二分类时返回形状（n_samples,），为每一个样本返回一个浮点数
	```
* 2）分析
值表示置信程度，正值偏正类，负值偏反类（其他类）

**2、预测概率**
* 1）使用方法
	```python
	model.perdict_proba(X_test)
	#二分类时返回（n_samples,2），分别为第一个元素概率和第二个元素概率
	```

**3、多分类问题的不确定度**
多分类问题的decision_function和perdict_proba形状都是（n_samples，n_classes）


## 第三章 无监督学习与预处理
### 一、预处理与缩放
**1、不同类型的预处理**
* 1）StandardScaler：确保每个特征平均值为零、方差为一
* 2）RobustScaler：与前者相似，但针对中位数和四分位数（会忽略异常值）
* 3）MinMaxScaler：移动数据使所有特征在零一之间（二位数据位于xy的零一矩形中）
* 4）Normalizer：对每个数据点缩放使特征向量的欧式长度等于一（投射到半径为1的圆/球面...上，适用于数据方向重要而特征向量长度不重要的缩放）

**2、应用数据变换**
* 1）操作方法
```python
	from sklearn.preprocessing import MinMaxScaler
	scaler = MinMaxScaler()
	scaler.fit(X_train)#与分类器和回归器不同，这里fit只用X_train而不用y_train
	X_train_scaled = scaler.transform(X_train)#应用刚刚学到的变换​
	X_test_scaled = scaler.transform(X_test)#这样同时缩放训练和测试数据，而不是分开fit再transform​
```
* 2）更为高效的方法：fit_transform方法链
	```python
	from sklearn.preprocessing import StandardScaler
	scaler=StandardScaler()
	X_scaled_d=scaler.fit_transform(X)
	```
### 二、降维、特征提取与流形学习
**1、主成分分析**
* 1）概述
旋转数据集的方法，旋转后的特征在统计上不相关
主成分：包含信息最多的方向（数据方差的主要方向）
主成分个数与原始特征数相同，可以通过仅保留一部分主成分来使用PCA进行降维
* 2）主成分提取
	* A.可视化技巧（二分类）：对每个特征分别计算两个类别的直方图（以三十个特征为例）
		```python
		fig, axes = plt.subplots(15, 2, figsize=(10, 20))
		#axes是一个包含subplots对象的numpy数组，可以用调用数组元素的方式对subplot进行索引
		malignant = cancer.data[cancer.target == 0]
		benign = cancer.data[cancer.target == 1]
		ax = axes.ravel()
		#ravel()变为一维数组，行序优先
		for i in range(30):
			_, bins = np.histogram(cancer.data[:, i], bins=50)
			ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)
			ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)
			ax[i].set_title(cancer.feature_names[i])
			ax[i].set_yticks(())
		ax[0].set_xlabel("Feature magnitude")
		ax[0].set_ylabel("Frequency")
		ax[0].legend(["malignant", "benign"], loc="best")
		fig.tight_layout()
		```
	* B.具体操作
		* a.基本方法
			```python
			from sklearn.decomposition import PCA
			pca = PCA(n_components=2)
			pca.fit(X_scaled)
			X_pca = pca.transform(X_scaled)
			```
		* b.参数
			* n_components：提取主成分个数
			* whiten：白化，将主成分缩放到相同尺度（如使用像素距离时，避免人脸位移对于人脸识别的影响）

**2、非负矩阵分解（NMF）**
* 1）概述
工作原理类似PCA，可用于降维
PCA中要正交分量，并且解释尽可能多的方差；而NMF中希望分量和系数均为非负
NMF得到的分量更容易解释（非负减少抵消效应）
* 2）具体操作
	```python
	from sklearn.decomposition import NMF
	nmf = NMF(n_components=15, random_state=0)
	nmf.fit(X_train)
	X_train_nmf = nmf.transform(X_train)
	X_test_nmf = nmf.transform(X_test)
	```

**3、用t-SNE进行流形学习**
* 1）概述
主要用处：探索性数据分析（制作可视化，而不是训练数据）
思想：找到数据的一个二维表示，尽可能保持数据点之间的距离（终点关注距离较近的点）
* 2）具体操作
	```python
	from sklearn.manifold import TSNE
	tsne = TSNE(random_state=42)
	digits_tsne = tsne.fit_transform(digits.data)
	#由于t-SNE不支持变换新数据，因此没有transform方法
	```
### 三、聚类
**1、k均值聚类**
* 1）原理
目标：找到代表数据特定区域的簇中心
过程：交替将每个数据点分配给最近的中心，再将中心设置为所分配的所有数据点的平均值
* 2）具体操作
	```python
	from sklearn.cluster import KMeans
	X, y = make_blobs(random_state=1)
	kmeans = KMeans(n_clusters=3)
	kmeans.fit(X)
	```
* 3）参数及属性
	* cluster_centers_属性：保存簇中心
	* kmeans.labels_属性：保存每个训练数据点的标签
	* n_clusters：簇中心个数
* 4）失败案例：簇密度不同、非球形簇、复杂形状簇

**2、凝聚聚类**
* 1）原理
	* 原则：算法声明每个点是自己的簇，然后合并两个最相似的簇，直到满足某种停止准则
	* 链接准则：规定如何度量最相似的簇
		* ward：默认选项，合并后方程增加最小
		* average：所有点之间平均距离最小的两个簇合并
		* complete：最大链接：将簇中点之间最大距离最小的两个簇合并
* 2）具体操作
	```python
	from sklearn.cluster import AgglomerativeClustering
	agg = AgglomerativeClustering(n_clusters=3)
	assignment = agg.fit_predict(X)
	#没有perdict方法，为了构造模型并得到训练集上簇的成员关系可以用fit_predict方法
	```
**3、DBSCAN（具有噪声的基于密度的空间聚类应用）**
* 1）原理及特点
不需要先验地设置簇的个数，还可以找出不属于任何簇的点
原理：识别空间中拥挤区域的点（dense zone，其中的点成为核心样本）
参数：min_sample、eps（若距一个数据点eps距离内至少有min_samples个数据点，就是核心样本）
分配在簇的eps内没有更多核心样本时停止
* 2）具体操作
	```python
	from sklearn.cluster import DBSCAN
	dbscan = DBSCAN()
	clusters = dbscan.fit_predict(X)
	```
**4、聚类算法的对比与评估**
* 1）用真实值评估聚类
	* A.指标（最佳值为1，0表示不相关聚类）
	调整rand指数（ARI）
	归一化信息（NMI）
	* B.具体操作
		```python
		from sklearn.metrics.cluster import adjusted_rand_score
		algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),DBSCAN()]
		clusters = algorithm.fit_predict(X_scaled)
		#此处放在for语句中实现​
		print(adjusted_rand_score(y, clusters))
		```
* 2）没有真实值情况评估聚类
	* A.轮廓系数
	计算一个簇的紧致程度，值越大越好（最大值为1）
	不允许复杂的形状
	* B.具体操作
		```python
		from sklearn.metrics.cluster import silhouette_score
		#其余操作同上
		```
* 3）小结
聚类的应用与评估->定性过程
k均值：指定簇数，平均值表示簇
DBSCAN：接近程度、检测噪声点、判断簇数量
凝聚聚类：指定簇数、划分层次
